import os
import json
import random
from typing import Dict, Any, Optional
from datetime import datetime
import pandas as pd
import openai
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from dotenv import load_dotenv
import ollama

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class MockLLM:
    """Mock LLM for testing without OpenAI API key"""
    
    def __init__(self):
        self.responses = {
            "bullish": [
                "åŸºäºæŠ€æœ¯åˆ†æï¼Œè¯¥è‚¡ç¥¨æ˜¾ç¤ºå‡ºå¼ºåŠ²çš„ä¸Šå‡è¶‹åŠ¿ã€‚RSIæŒ‡æ ‡æ˜¾ç¤ºè¶…å–ååå¼¹ï¼ŒMACDå‡ºç°é‡‘å‰ä¿¡å·ï¼Œæˆäº¤é‡æ”¾å¤§ï¼Œé¢„è®¡çŸ­æœŸå†…å°†ç»§ç»­ä¸Šæ¶¨ã€‚",
                "ä»åŸºæœ¬é¢å’ŒæŠ€æœ¯é¢åˆ†æï¼Œè¯¥è‚¡ç¥¨å…·æœ‰è‰¯å¥½çš„ä¸Šæ¶¨æ½œåŠ›ã€‚ç§»åŠ¨å¹³å‡çº¿å‘ˆå¤šå¤´æ’åˆ—ï¼Œå¸ƒæ—å¸¦æ”¶çª„åçªç ´ä¸Šè½¨ï¼Œå»ºè®®å…³æ³¨ã€‚",
                "æŠ€æœ¯æŒ‡æ ‡æ˜¾ç¤ºè¯¥è‚¡ç¥¨å¤„äºä¸Šå‡é€šé“ä¸­ï¼Œæ”¯æ’‘ä½ç¨³å›ºï¼Œé˜»åŠ›ä½æœ‰æœ›çªç ´ã€‚æˆäº¤é‡é…åˆè‰¯å¥½ï¼Œä¸Šæ¶¨æ¦‚ç‡è¾ƒé«˜ã€‚"
            ],
            "bearish": [
                "æŠ€æœ¯åˆ†ææ˜¾ç¤ºè¯¥è‚¡ç¥¨é¢ä¸´ä¸‹è¡Œå‹åŠ›ã€‚RSIæŒ‡æ ‡è¶…ä¹°ï¼ŒMACDå‡ºç°æ­»å‰ï¼Œæˆäº¤é‡èç¼©ï¼Œé¢„è®¡çŸ­æœŸå†…å¯èƒ½å›è°ƒã€‚",
                "ä»æŠ€æœ¯é¢çœ‹ï¼Œè¯¥è‚¡ç¥¨å·²æ¥è¿‘é˜»åŠ›ä½ï¼Œç§»åŠ¨å¹³å‡çº¿å¼€å§‹èµ°å¹³ï¼Œå¸ƒæ—å¸¦æ”¶çª„ï¼ŒçŸ­æœŸå†…ä¸Šæ¶¨ç©ºé—´æœ‰é™ã€‚",
                "æŠ€æœ¯æŒ‡æ ‡æ˜¾ç¤ºè¯¥è‚¡ç¥¨å¯èƒ½é¢ä¸´è°ƒæ•´ã€‚æ”¯æ’‘ä½è¾ƒå¼±ï¼Œæˆäº¤é‡ä¸è¶³ï¼Œå»ºè®®è°¨æ…æ“ä½œã€‚"
            ],
            "neutral": [
                "æŠ€æœ¯åˆ†ææ˜¾ç¤ºè¯¥è‚¡ç¥¨å¤„äºæ¨ªç›˜æ•´ç†çŠ¶æ€ã€‚å„é¡¹æŒ‡æ ‡ç›¸å¯¹ä¸­æ€§ï¼ŒçŸ­æœŸå†…å¯èƒ½ç»´æŒéœ‡è¡èµ°åŠ¿ã€‚",
                "ä»æŠ€æœ¯é¢çœ‹ï¼Œè¯¥è‚¡ç¥¨ç¼ºä¹æ˜ç¡®æ–¹å‘æ€§ä¿¡å·ã€‚å¤šç©ºåŠ›é‡ç›¸å¯¹å‡è¡¡ï¼Œå»ºè®®è§‚æœ›ç­‰å¾…çªç ´ã€‚",
                "æŠ€æœ¯æŒ‡æ ‡æ˜¾ç¤ºè¯¥è‚¡ç¥¨å¤„äºå…³é”®ä½ç½®ï¼Œéœ€è¦æ›´å¤šä¿¡å·ç¡®è®¤æ–¹å‘ã€‚å»ºè®®å…³æ³¨åç»­èµ°åŠ¿ã€‚"
            ]
        }
    
    def analyze_stock(self, symbol: str, data: Dict[str, Any], timeframe: str) -> Dict[str, Any]:
        """åˆ†æè‚¡ç¥¨å¹¶è¿”å›é¢„æµ‹ç»“æœ"""
        # åŸºäºæŠ€æœ¯æŒ‡æ ‡ç”Ÿæˆæ¨¡æ‹Ÿåˆ†æ
        indicators = data.get('indicators', {})
        signal_strength = data.get('signal_strength', {})
        
        # æ ¹æ®ä¿¡å·å¼ºåº¦ç¡®å®šæ–¹å‘
        strength = signal_strength.get('strength', 'neutral')
        
        if 'bullish' in strength:
            direction = 'up'
            probability = random.uniform(60, 85)
            price_change = random.uniform(2, 8)
        elif 'bearish' in strength:
            direction = 'down'
            probability = random.uniform(60, 85)
            price_change = random.uniform(-8, -2)
        else:
            direction = 'neutral'
            probability = random.uniform(45, 55)
            price_change = random.uniform(-2, 2)
        
        # é€‰æ‹©å¯¹åº”çš„åˆ†ææ–‡æœ¬
        analysis_text = random.choice(self.responses.get(direction, self.responses['neutral']))
        
        return {
            "direction": direction,
            "probability": round(probability, 1),
            "price_change_percent": round(price_change, 1),
            "reasoning": analysis_text,
            "confidence": "medium",
            "risk_factors": [
                "å¸‚åœºæ³¢åŠ¨æ€§è¾ƒé«˜",
                "æŠ€æœ¯æŒ‡æ ‡å¯èƒ½å­˜åœ¨æ»åæ€§",
                "åŸºæœ¬é¢å› ç´ æœªè€ƒè™‘åœ¨å†…"
            ]
        }
    
    def generate_top_stocks(self, market_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆTop 10è‚¡ç¥¨å»ºè®®"""
        # æ¨¡æ‹Ÿçš„è‚¡ç¥¨åˆ—è¡¨
        mock_stocks = [
            {"symbol": "AAPL", "name": "Apple Inc.", "sector": "Technology"},
            {"symbol": "MSFT", "name": "Microsoft Corporation", "sector": "Technology"},
            {"symbol": "GOOGL", "name": "Alphabet Inc.", "sector": "Technology"},
            {"symbol": "AMZN", "name": "Amazon.com Inc.", "sector": "Consumer Discretionary"},
            {"symbol": "TSLA", "name": "Tesla Inc.", "sector": "Consumer Discretionary"},
            {"symbol": "NVDA", "name": "NVIDIA Corporation", "sector": "Technology"},
            {"symbol": "META", "name": "Meta Platforms Inc.", "sector": "Technology"},
            {"symbol": "NFLX", "name": "Netflix Inc.", "sector": "Communication Services"},
            {"symbol": "AMD", "name": "Advanced Micro Devices", "sector": "Technology"},
            {"symbol": "CRM", "name": "Salesforce Inc.", "sector": "Technology"}
        ]
        
        recommendations = []
        for stock in mock_stocks:
            direction = random.choice(['up', 'down', 'neutral'])
            probability = random.uniform(55, 80)
            
            if direction == 'up':
                reasoning = f"{stock['name']}åœ¨{stock['sector']}é¢†åŸŸå…·æœ‰å¼ºåŠ²çš„åŸºæœ¬é¢å’ŒæŠ€æœ¯é¢æ”¯æ’‘ï¼Œé¢„è®¡çŸ­æœŸå†…è¡¨ç°è‰¯å¥½ã€‚"
                risk_level = "medium"
                expected_return = f"+{random.uniform(3, 12):.1f}%"
            elif direction == 'down':
                reasoning = f"{stock['name']}é¢ä¸´ä¸€äº›æŒ‘æˆ˜ï¼ŒæŠ€æœ¯æŒ‡æ ‡æ˜¾ç¤ºå¯èƒ½å›è°ƒï¼Œå»ºè®®è°¨æ…ã€‚"
                risk_level = "high"
                expected_return = f"{random.uniform(-8, -2):.1f}%"
            else:
                reasoning = f"{stock['name']}å¤„äºæ¨ªç›˜æ•´ç†çŠ¶æ€ï¼Œéœ€è¦æ›´å¤šä¿¡å·ç¡®è®¤æ–¹å‘ã€‚"
                risk_level = "low"
                expected_return = f"{random.uniform(-2, 3):.1f}%"
            
            recommendations.append({
                "symbol": stock['symbol'],
                "name": stock['name'],
                "direction": direction,
                "probability": round(probability, 1),
                "reasoning": reasoning,
                "risk_level": risk_level,
                "expected_return": expected_return
            })
        
        return {
            "recommendations": recommendations,
            "generated_at": datetime.now().isoformat(),
            "disclaimer": "æœ¬å»ºè®®ä»…ç”¨äºå­¦ä¹ ç ”ç©¶ç›®çš„ï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚æŠ•èµ„æœ‰é£é™©ï¼Œå…¥å¸‚éœ€è°¨æ…ã€‚"
        }


class OllamaLLM:
    """Ollama æœ¬åœ° LLM åˆ†æå™¨"""
    
    def __init__(self, model_name: str = "qwen2.5:7b"):
        self.model_name = model_name
        self.llm = None
        self._initialize_llm()
    
    def _initialize_llm(self):
        """åˆå§‹åŒ–Ollama LLMå®ä¾‹"""
        try:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
            models = ollama.list()
            available_models = [model.model for model in models.models]
            
            if self.model_name not in available_models:
                print(f"âŒ æ¨¡å‹ {self.model_name} æœªæ‰¾åˆ°ï¼Œå¯ç”¨æ¨¡å‹: {available_models}")
                self.llm = None
                return
            
            # æµ‹è¯•è¿æ¥
            test_response = ollama.chat(
                model=self.model_name,
                messages=[{'role': 'user', 'content': 'Hello'}]
            )
            
            self.llm = True  # æ ‡è®°ä¸ºå¯ç”¨
            print(f"âœ… Ollamaæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {self.model_name}")
            
        except Exception as e:
            print(f"âŒ Ollamaæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.llm = None
    
    def _check_and_reinitialize(self):
        """æ£€æŸ¥å¹¶é‡æ–°åˆå§‹åŒ–LLM"""
        if not self.llm:
            print("ğŸ”„ é‡æ–°åˆå§‹åŒ–Ollamaæ¨¡å‹...")
            self._initialize_llm()
    
    def analyze_stock(self, symbol: str, data: Dict[str, Any], timeframe: str) -> Dict[str, Any]:
        """ä½¿ç”¨ Ollama åˆ†æè‚¡ç¥¨"""
        # æ£€æŸ¥å¹¶é‡æ–°åˆå§‹åŒ–LLM
        self._check_and_reinitialize()
        
        if not self.llm:
            return {
                "error": "Ollama model not available",
                "direction": None,
                "probability": None,
                "price_change_percent": None,
                "reasoning": None,
                "confidence": None
            }
        
        try:
            # æ„å»ºåˆ†ææç¤º
            indicators_summary = self._format_indicators(data.get('indicators', {}))
            signal_info = data.get('signal_strength', {})
            
            prompt = f"""
ä½œä¸ºä¸“ä¸šçš„è‚¡ç¥¨åˆ†æå¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹æŠ€æœ¯æŒ‡æ ‡æ•°æ®å¯¹è‚¡ç¥¨ {symbol} è¿›è¡Œ {timeframe} æ—¶é—´æ¡†æ¶çš„åˆ†æï¼š

æŠ€æœ¯æŒ‡æ ‡æ‘˜è¦ï¼š
{indicators_summary}

ä¿¡å·å¼ºåº¦ï¼š{signal_info.get('strength', 'neutral')}
ä¿¡å·å¾—åˆ†ï¼š{signal_info.get('score', 0)}

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- direction: "up", "down", æˆ– "neutral"
- probability: 0-100ä¹‹é—´çš„æ•°å­—ï¼Œè¡¨ç¤ºé¢„æµ‹å‡†ç¡®æ€§çš„æ¦‚ç‡
- price_change_percent: é¢„æœŸä»·æ ¼å˜åŒ–ç™¾åˆ†æ¯”
- reasoning: è¯¦ç»†çš„åˆ†æç†ç”±ï¼ˆä¸­æ–‡ï¼‰
- confidence: "high", "medium", æˆ– "low"
- risk_factors: é£é™©å› ç´ åˆ—è¡¨

æ³¨æ„ï¼šè¯·ä¿æŒå®¢è§‚å’Œè°¨æ…ï¼Œè€ƒè™‘å¸‚åœºé£é™©ã€‚
"""
            
            response = ollama.chat(
                model=self.model_name,
                messages=[
                    {'role': 'system', 'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‚¡ç¥¨æŠ€æœ¯åˆ†æå¸ˆï¼Œæ“…é•¿åŸºäºæŠ€æœ¯æŒ‡æ ‡è¿›è¡Œè‚¡ç¥¨èµ°åŠ¿é¢„æµ‹ã€‚'},
                    {'role': 'user', 'content': prompt}
                ]
            )
            
            # è§£æå“åº”
            try:
                result = json.loads(response['message']['content'])
                return result
            except json.JSONDecodeError:
                # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æœ
                return {
                    "direction": "neutral",
                    "probability": 50.0,
                    "price_change_percent": 0.0,
                    "reasoning": "æŠ€æœ¯åˆ†æç»“æœè§£æå¤±è´¥ï¼Œå»ºè®®è°¨æ…æ“ä½œã€‚",
                    "confidence": "low",
                    "risk_factors": ["æ•°æ®è§£æå¼‚å¸¸"]
                }
                
        except Exception as e:
            print(f"Ollama API error: {e}")
            return {
                "error": "Ollama model not available",
                "direction": None,
                "probability": None,
                "price_change_percent": None,
                "reasoning": None,
                "confidence": None
            }
    
    def generate_top_stocks(self, market_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆTop 10è‚¡ç¥¨å»ºè®®"""
        # æ£€æŸ¥å¹¶é‡æ–°åˆå§‹åŒ–LLM
        self._check_and_reinitialize()
        
        if not self.llm:
            return {
                "error": "Ollama model not available",
                "recommendations": [],
                "generated_at": None,
                "disclaimer": None
            }
        
        try:
            prompt = f"""
ä½œä¸ºä¸“ä¸šçš„æŠ•èµ„é¡¾é—®ï¼Œè¯·åŸºäºå½“å‰å¸‚åœºæƒ…å†µæ¨è10åªå…·æœ‰æŠ•èµ„æ½œåŠ›çš„è‚¡ç¥¨ã€‚

è¯·ä»¥JSONæ ¼å¼è¿”å›æ¨èç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- recommendations: åŒ…å«10ä¸ªè‚¡ç¥¨å¯¹è±¡çš„æ•°ç»„ï¼Œæ¯ä¸ªå¯¹è±¡åŒ…å«ï¼š
  - symbol: è‚¡ç¥¨ä»£ç 
  - name: å…¬å¸åç§°
  - direction: "up", "down", æˆ– "neutral"
  - probability: 0-100ä¹‹é—´çš„æ•°å­—
  - reasoning: æ¨èç†ç”±ï¼ˆä¸­æ–‡ï¼‰
  - risk_level: "low", "medium", æˆ– "high"
  - expected_return: é¢„æœŸæ”¶ç›Šç‡ï¼ˆå¦‚"+5.2%"ï¼‰
- generated_at: ç”Ÿæˆæ—¶é—´
- disclaimer: å…è´£å£°æ˜

æ³¨æ„ï¼šè¯·é€‰æ‹©çŸ¥åçš„å¤§ç›˜è‚¡ï¼Œå¹¶ä¿æŒå®¢è§‚åˆ†æã€‚
"""
            
            response = ollama.chat(
                model=self.model_name,
                messages=[
                    {'role': 'system', 'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ•èµ„é¡¾é—®ï¼Œæ“…é•¿è‚¡ç¥¨åˆ†æå’ŒæŠ•èµ„å»ºè®®ã€‚'},
                    {'role': 'user', 'content': prompt}
                ]
            )
            
            try:
                result = json.loads(response['message']['content'])
                return result
            except json.JSONDecodeError:
                return {
                    "error": "Ollama model not available",
                    "recommendations": [],
                    "generated_at": None,
                    "disclaimer": None
                }
                
        except Exception as e:
            print(f"Ollama API error: {e}")
            return {
                "error": "Ollama model not available",
                "recommendations": [],
                "generated_at": None,
                "disclaimer": None
            }
    
    def _format_indicators(self, indicators: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æŠ€æœ¯æŒ‡æ ‡æ•°æ®"""
        if not indicators:
            return "æ— æŠ€æœ¯æŒ‡æ ‡æ•°æ®"
        
        summary = []
        for key, value in indicators.items():
            if isinstance(value, (int, float)) and not pd.isna(value):
                summary.append(f"{key}: {value:.2f}")
            elif hasattr(value, 'iloc') and len(value) > 0:
                summary.append(f"{key}: {value.iloc[-1]:.2f}")
        
        return "\n".join(summary[:10])  # é™åˆ¶æ˜¾ç¤ºå‰10ä¸ªæŒ‡æ ‡


class OpenAILLM:
    """OpenAI LLM åˆ†æå™¨"""
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.llm = None
        self._initialize_llm()
    
    def _initialize_llm(self):
        """åˆå§‹åŒ–LLMå®ä¾‹"""
        if self.api_key and not self.llm:
            try:
                openai.api_key = self.api_key
                self.llm = ChatOpenAI(
                    model="gpt-3.5-turbo",
                    temperature=0.3,
                    max_tokens=1000
                )
                print(f"âœ… GPTæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {self.llm.model_name}")
            except Exception as e:
                print(f"âŒ GPTæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                self.llm = None
        elif not self.api_key:
            print("âš ï¸ æœªé…ç½®OpenAI APIå¯†é’¥")
            self.llm = None
    
    def _check_and_reinitialize(self):
        """æ£€æŸ¥å¹¶é‡æ–°åˆå§‹åŒ–LLM"""
        if not self.llm and self.api_key:
            print("ğŸ”„ é‡æ–°åˆå§‹åŒ–GPTæ¨¡å‹...")
            self._initialize_llm()
    
    def analyze_stock(self, symbol: str, data: Dict[str, Any], timeframe: str) -> Dict[str, Any]:
        """ä½¿ç”¨ OpenAI åˆ†æè‚¡ç¥¨"""
        # æ£€æŸ¥å¹¶é‡æ–°åˆå§‹åŒ–LLM
        self._check_and_reinitialize()
        
        if not self.llm:
            # å¦‚æœæ²¡æœ‰ API keyï¼Œè¿”å›é”™è¯¯
            return {
                "error": "GPT model not work, figure out how to fix it",
                "direction": None,
                "probability": None,
                "price_change_percent": None,
                "reasoning": None,
                "confidence": None
            }
        
        try:
            # æ„å»ºåˆ†ææç¤º
            indicators_summary = self._format_indicators(data.get('indicators', {}))
            signal_info = data.get('signal_strength', {})
            
            prompt = f"""
            ä½œä¸ºä¸“ä¸šçš„è‚¡ç¥¨åˆ†æå¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹æŠ€æœ¯æŒ‡æ ‡æ•°æ®å¯¹è‚¡ç¥¨ {symbol} è¿›è¡Œ {timeframe} æ—¶é—´æ¡†æ¶çš„åˆ†æï¼š

            æŠ€æœ¯æŒ‡æ ‡æ‘˜è¦ï¼š
            {indicators_summary}

            ä¿¡å·å¼ºåº¦ï¼š{signal_info.get('strength', 'neutral')}
            ä¿¡å·å¾—åˆ†ï¼š{signal_info.get('score', 0)}

            è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - direction: "up", "down", æˆ– "neutral"
            - probability: 0-100ä¹‹é—´çš„æ•°å­—ï¼Œè¡¨ç¤ºé¢„æµ‹å‡†ç¡®æ€§çš„æ¦‚ç‡
            - price_change_percent: é¢„æœŸä»·æ ¼å˜åŒ–ç™¾åˆ†æ¯”
            - reasoning: è¯¦ç»†çš„åˆ†æç†ç”±ï¼ˆä¸­æ–‡ï¼‰
            - confidence: "high", "medium", æˆ– "low"
            - risk_factors: é£é™©å› ç´ åˆ—è¡¨

            æ³¨æ„ï¼šè¯·ä¿æŒå®¢è§‚å’Œè°¨æ…ï¼Œè€ƒè™‘å¸‚åœºé£é™©ã€‚
            """
            
            messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‚¡ç¥¨æŠ€æœ¯åˆ†æå¸ˆï¼Œæ“…é•¿åŸºäºæŠ€æœ¯æŒ‡æ ‡è¿›è¡Œè‚¡ç¥¨èµ°åŠ¿é¢„æµ‹ã€‚"),
                HumanMessage(content=prompt)
            ]
            
            response = self.llm(messages)
            
            # è§£æå“åº”
            try:
                result = json.loads(response.content)
                return result
            except json.JSONDecodeError:
                # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æœ
                return {
                    "direction": "neutral",
                    "probability": 50.0,
                    "price_change_percent": 0.0,
                    "reasoning": "æŠ€æœ¯åˆ†æç»“æœè§£æå¤±è´¥ï¼Œå»ºè®®è°¨æ…æ“ä½œã€‚",
                    "confidence": "low",
                    "risk_factors": ["æ•°æ®è§£æå¼‚å¸¸"]
                }
                
        except Exception as e:
            print(f"OpenAI API error: {e}")
            # å‡ºé”™æ—¶è¿”å›é”™è¯¯ä¿¡æ¯
            return {
                "error": "GPT model not work, figure out how to fix it",
                "direction": None,
                "probability": None,
                "price_change_percent": None,
                "reasoning": None,
                "confidence": None
            }
    
    def generate_top_stocks(self, market_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆTop 10è‚¡ç¥¨å»ºè®®"""
        # æ£€æŸ¥å¹¶é‡æ–°åˆå§‹åŒ–LLM
        self._check_and_reinitialize()
        
        if not self.llm:
            return {
                "error": "GPT model not work, figure out how to fix it",
                "recommendations": [],
                "generated_at": None,
                "disclaimer": None
            }
        
        try:
            prompt = f"""
            ä½œä¸ºä¸“ä¸šçš„æŠ•èµ„é¡¾é—®ï¼Œè¯·åŸºäºå½“å‰å¸‚åœºæƒ…å†µæ¨è10åªå…·æœ‰æŠ•èµ„æ½œåŠ›çš„è‚¡ç¥¨ã€‚

            è¯·ä»¥JSONæ ¼å¼è¿”å›æ¨èç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - recommendations: åŒ…å«10ä¸ªè‚¡ç¥¨å¯¹è±¡çš„æ•°ç»„ï¼Œæ¯ä¸ªå¯¹è±¡åŒ…å«ï¼š
              - symbol: è‚¡ç¥¨ä»£ç 
              - name: å…¬å¸åç§°
              - direction: "up", "down", æˆ– "neutral"
              - probability: 0-100ä¹‹é—´çš„æ•°å­—
              - reasoning: æ¨èç†ç”±ï¼ˆä¸­æ–‡ï¼‰
              - risk_level: "low", "medium", æˆ– "high"
              - expected_return: é¢„æœŸæ”¶ç›Šç‡ï¼ˆå¦‚"+5.2%"ï¼‰
            - generated_at: ç”Ÿæˆæ—¶é—´
            - disclaimer: å…è´£å£°æ˜

            æ³¨æ„ï¼šè¯·é€‰æ‹©çŸ¥åçš„å¤§ç›˜è‚¡ï¼Œå¹¶ä¿æŒå®¢è§‚åˆ†æã€‚
            """
            
            messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ•èµ„é¡¾é—®ï¼Œæ“…é•¿è‚¡ç¥¨åˆ†æå’ŒæŠ•èµ„å»ºè®®ã€‚"),
                HumanMessage(content=prompt)
            ]
            
            response = self.llm(messages)
            
            try:
                result = json.loads(response.content)
                return result
            except json.JSONDecodeError:
                return {
                    "error": "GPT model not work, figure out how to fix it",
                    "recommendations": [],
                    "generated_at": None,
                    "disclaimer": None
                }
                
        except Exception as e:
            print(f"OpenAI API error: {e}")
            # å‡ºé”™æ—¶è¿”å›é”™è¯¯ä¿¡æ¯
            return {
                "error": "GPT model not work, figure out how to fix it",
                "recommendations": [],
                "generated_at": None,
                "disclaimer": None
            }
    
    def _format_indicators(self, indicators: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æŠ€æœ¯æŒ‡æ ‡æ•°æ®"""
        if not indicators:
            return "æ— æŠ€æœ¯æŒ‡æ ‡æ•°æ®"
        
        summary = []
        for key, value in indicators.items():
            if isinstance(value, (int, float)) and not pd.isna(value):
                summary.append(f"{key}: {value:.2f}")
            elif hasattr(value, 'iloc') and len(value) > 0:
                summary.append(f"{key}: {value.iloc[-1]:.2f}")
        
        return "\n".join(summary[:10])  # é™åˆ¶æ˜¾ç¤ºå‰10ä¸ªæŒ‡æ ‡


# å…¨å±€ LLM å®ä¾‹ - æ ¹æ®ç¯å¢ƒå˜é‡é€‰æ‹©æ¨¡å‹
def get_llm_analyzer():
    """æ ¹æ®ç¯å¢ƒå˜é‡é€‰æ‹©LLMåˆ†æå™¨"""
    llm_type = os.getenv("LLM_TYPE", "openai").lower()
    
    if llm_type == "ollama":
        model_name = os.getenv("OLLAMA_MODEL", "qwen2.5:7b")
        return OllamaLLM(model_name=model_name)
    elif llm_type == "mock":
        return MockLLM()
    else:
        return OpenAILLM()

# å…¨å±€ LLM å®ä¾‹
llm_analyzer = get_llm_analyzer()
