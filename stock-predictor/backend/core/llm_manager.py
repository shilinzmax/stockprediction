"""
LLMç®¡ç†å™¨ - å…¨å±€å•ä¾‹æ¨¡å¼ç®¡ç†GPTæ¨¡å‹
"""
import os
from typing import Optional
from dotenv import load_dotenv
from .llm import OpenAILLM, OllamaLLM, MockLLM, get_llm_analyzer

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class LLMManager:
    """LLMç®¡ç†å™¨ - å•ä¾‹æ¨¡å¼"""
    
    _instance = None
    _llm_analyzer = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(LLMManager, cls).__new__(cls)
            cls._instance._initialize()
        return cls._instance
    
    def _initialize(self):
        """åˆå§‹åŒ–LLMåˆ†æå™¨"""
        if self._llm_analyzer is None:
            print("ğŸš€ åˆå§‹åŒ–å…¨å±€LLMç®¡ç†å™¨...")
            self._llm_analyzer = get_llm_analyzer()
            print("âœ… å…¨å±€LLMç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def get_llm_analyzer(self):
        """è·å–LLMåˆ†æå™¨å®ä¾‹"""
        return self._llm_analyzer
    
    def is_llm_available(self) -> bool:
        """æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨"""
        if self._llm_analyzer is None:
            return False
        
        # æ ¹æ®LLMç±»å‹æ£€æŸ¥å¯ç”¨æ€§
        if isinstance(self._llm_analyzer, OpenAILLM):
            return self._llm_analyzer.llm is not None
        elif isinstance(self._llm_analyzer, OllamaLLM):
            return self._llm_analyzer.llm is not None
        elif isinstance(self._llm_analyzer, MockLLM):
            return True
        
        return False
    
    def get_llm_status(self) -> dict:
        """è·å–LLMçŠ¶æ€ä¿¡æ¯"""
        if self._llm_analyzer is None:
            return {
                "available": False,
                "reason": "LLMåˆ†æå™¨æœªåˆå§‹åŒ–",
                "type": "unknown"
            }
        
        llm_type = type(self._llm_analyzer).__name__
        
        if isinstance(self._llm_analyzer, OpenAILLM):
            if not self._llm_analyzer.api_key:
                return {
                    "available": False,
                    "reason": "æœªé…ç½®OpenAI APIå¯†é’¥",
                    "type": "openai"
                }
            
            if not self._llm_analyzer.llm:
                return {
                    "available": False,
                    "reason": "GPTæ¨¡å‹åˆå§‹åŒ–å¤±è´¥",
                    "type": "openai"
                }
            
            return {
                "available": True,
                "type": "openai",
                "model": self._llm_analyzer.llm.model_name,
                "temperature": self._llm_analyzer.llm.temperature,
                "api_key_configured": bool(self._llm_analyzer.api_key)
            }
        
        elif isinstance(self._llm_analyzer, OllamaLLM):
            if not self._llm_analyzer.llm:
                return {
                    "available": False,
                    "reason": "Ollamaæ¨¡å‹åˆå§‹åŒ–å¤±è´¥",
                    "type": "ollama"
                }
            
            return {
                "available": True,
                "type": "ollama",
                "model": self._llm_analyzer.model_name,
                "api_key_configured": False
            }
        
        elif isinstance(self._llm_analyzer, MockLLM):
            return {
                "available": True,
                "type": "mock",
                "model": "Mock LLM",
                "api_key_configured": False
            }
        
        return {
            "available": False,
            "reason": "æœªçŸ¥çš„LLMç±»å‹",
            "type": llm_type
        }


# å…¨å±€LLMç®¡ç†å™¨å®ä¾‹
llm_manager = LLMManager()

# ä¾¿æ·å‡½æ•°
def get_llm_analyzer():
    """è·å–LLMåˆ†æå™¨å®ä¾‹"""
    return llm_manager.get_llm_analyzer()

def is_llm_available() -> bool:
    """æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨"""
    return llm_manager.is_llm_available()

def get_llm_status() -> dict:
    """è·å–LLMçŠ¶æ€ä¿¡æ¯"""
    return llm_manager.get_llm_status()

# å‘åå…¼å®¹çš„å‡½æ•°
def is_gpt_available() -> bool:
    """æ£€æŸ¥GPTæ˜¯å¦å¯ç”¨ï¼ˆå‘åå…¼å®¹ï¼‰"""
    return llm_manager.is_llm_available()

def get_gpt_status() -> dict:
    """è·å–GPTçŠ¶æ€ä¿¡æ¯ï¼ˆå‘åå…¼å®¹ï¼‰"""
    return llm_manager.get_llm_status()
