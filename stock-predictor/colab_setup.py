#!/usr/bin/env python3
"""
Google Colab ç¯å¢ƒè®¾ç½®è„šæœ¬
ä¸“ä¸ºåœ¨Colabä¸­è¿è¡Œè‚¡ç¥¨é¢„æµ‹ç³»ç»Ÿè€Œè®¾è®¡
"""

import os
import sys
import subprocess
import time
import threading
import requests
from pathlib import Path

def install_system_dependencies():
    """å®‰è£…ç³»ç»Ÿä¾èµ–"""
    print("ğŸ”§ å®‰è£…ç³»ç»Ÿä¾èµ–...")
    
    commands = [
        "apt-get update",
        "apt-get install -y curl wget",
        "curl -fsSL https://ollama.com/install.sh | sh"
    ]
    
    for cmd in commands:
        try:
            result = subprocess.run(cmd, shell=True, check=True, capture_output=True, text=True)
            print(f"âœ… {cmd} å®Œæˆ")
        except subprocess.CalledProcessError as e:
            print(f"âŒ {cmd} å¤±è´¥: {e}")
            return False
    
    return True

def install_python_dependencies():
    """å®‰è£…Pythonä¾èµ–"""
    print("ğŸ“¦ å®‰è£…Pythonä¾èµ–...")
    
    packages = [
        "fastapi",
        "uvicorn[standard]",
        "langgraph",
        "langchain",
        "langchain-openai",
        "yfinance",
        "pandas",
        "numpy",
        "ta",
        "pydantic",
        "python-multipart",
        "python-dotenv",
        "httpx",
        "aiofiles",
        "pyarrow",
        "alpha-vantage",
        "requests",
        "aiohttp",
        "ollama",
        "jupyter-dash",
        "plotly",
        "dash"
    ]
    
    try:
        for package in packages:
            subprocess.run([sys.executable, "-m", "pip", "install", "-q", package], check=True)
        print("âœ… Pythonä¾èµ–å®‰è£…å®Œæˆ")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ Pythonä¾èµ–å®‰è£…å¤±è´¥: {e}")
        return False

def start_ollama_service():
    """å¯åŠ¨OllamaæœåŠ¡"""
    print("ğŸ¤– å¯åŠ¨OllamaæœåŠ¡...")
    
    def run_ollama():
        try:
            subprocess.run(['ollama', 'serve'], check=True)
        except subprocess.CalledProcessError as e:
            print(f"âŒ OllamaæœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
    
    # åœ¨åå°å¯åŠ¨Ollama
    ollama_thread = threading.Thread(target=run_ollama, daemon=True)
    ollama_thread.start()
    
    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    time.sleep(10)
    
    # æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯åŠ¨
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            print("âœ… OllamaæœåŠ¡å¯åŠ¨æˆåŠŸ")
            return True
    except:
        pass
    
    print("âŒ OllamaæœåŠ¡å¯åŠ¨å¤±è´¥")
    return False

def download_llm_model(model_name="qwen2.5:7b"):
    """ä¸‹è½½LLMæ¨¡å‹"""
    print(f"ğŸ“¥ ä¸‹è½½æ¨¡å‹: {model_name}")
    
    try:
        result = subprocess.run(['ollama', 'pull', model_name], check=True, capture_output=True, text=True)
        print(f"âœ… æ¨¡å‹ {model_name} ä¸‹è½½å®Œæˆ")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
        return False

def setup_environment():
    """è®¾ç½®ç¯å¢ƒå˜é‡"""
    print("âš™ï¸ é…ç½®ç¯å¢ƒå˜é‡...")
    
    env_vars = {
        "LLM_TYPE": "ollama",
        "OLLAMA_MODEL": "qwen2.5:7b",
        "HOST": "0.0.0.0",
        "PORT": "8000",
        "CACHE_TTL_HOURS": "24",
        "LOG_LEVEL": "INFO"
    }
    
    for key, value in env_vars.items():
        os.environ[key] = value
        print(f"  {key}={value}")
    
    print("âœ… ç¯å¢ƒå˜é‡é…ç½®å®Œæˆ")

def test_ollama_connection():
    """æµ‹è¯•Ollamaè¿æ¥"""
    print("ğŸ§ª æµ‹è¯•Ollamaè¿æ¥...")
    
    try:
        import ollama
        
        response = ollama.chat(
            model="qwen2.5:7b",
            messages=[{'role': 'user', 'content': 'Hello, are you working?'}]
        )
        
        print("âœ… Ollamaæµ‹è¯•æˆåŠŸ")
        print(f"å›å¤: {response['message']['content'][:100]}...")
        return True
        
    except Exception as e:
        print(f"âŒ Ollamaæµ‹è¯•å¤±è´¥: {e}")
        return False

def create_colab_notebook():
    """åˆ›å»ºColabæ¼”ç¤ºnotebook"""
    print("ğŸ““ åˆ›å»ºColabæ¼”ç¤ºnotebook...")
    
    notebook_content = '''{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è‚¡ç¥¨é¢„æµ‹ç³»ç»Ÿ - Colabæ¼”ç¤º\\n",
    "\\n",
    "è¿™ä¸ªnotebookæ¼”ç¤ºäº†å¦‚ä½•åœ¨Google Colabä¸­è¿è¡Œè‚¡ç¥¨é¢„æµ‹ç³»ç»Ÿã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡Œè®¾ç½®è„šæœ¬\\n",
    "!python colab_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯åŠ¨åº”ç”¨\\n",
    "import uvicorn\\n",
    "from backend.app import app\\n",
    "\\n",
    "uvicorn.run(app, host=\\"0.0.0.0\\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}'''
    
    with open("stock_predictor_demo.ipynb", "w") as f:
        f.write(notebook_content)
    
    print("âœ… Colabæ¼”ç¤ºnotebookåˆ›å»ºå®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è®¾ç½®Google Colabç¯å¢ƒ...")
    print("=" * 50)
    
    # æ£€æŸ¥æ˜¯å¦åœ¨Colabç¯å¢ƒä¸­
    try:
        import google.colab
        print("âœ… æ£€æµ‹åˆ°Colabç¯å¢ƒ")
    except ImportError:
        print("âš ï¸ æœªæ£€æµ‹åˆ°Colabç¯å¢ƒï¼Œä½†è„šæœ¬ä»å¯è¿è¡Œ")
    
    # æ‰§è¡Œè®¾ç½®æ­¥éª¤
    steps = [
        ("å®‰è£…ç³»ç»Ÿä¾èµ–", install_system_dependencies),
        ("å®‰è£…Pythonä¾èµ–", install_python_dependencies),
        ("å¯åŠ¨OllamaæœåŠ¡", start_ollama_service),
        ("ä¸‹è½½LLMæ¨¡å‹", lambda: download_llm_model("qwen2.5:7b")),
        ("é…ç½®ç¯å¢ƒå˜é‡", setup_environment),
        ("æµ‹è¯•Ollamaè¿æ¥", test_ollama_connection),
        ("åˆ›å»ºæ¼”ç¤ºnotebook", create_colab_notebook)
    ]
    
    success_count = 0
    for step_name, step_func in steps:
        print(f"\\nğŸ“‹ {step_name}...")
        try:
            if step_func():
                success_count += 1
            else:
                print(f"âŒ {step_name} å¤±è´¥")
        except Exception as e:
            print(f"âŒ {step_name} å¼‚å¸¸: {e}")
    
    print("\\n" + "=" * 50)
    print(f"ğŸ‰ è®¾ç½®å®Œæˆï¼æˆåŠŸæ­¥éª¤: {success_count}/{len(steps)}")
    
    if success_count == len(steps):
        print("\\nâœ… æ‰€æœ‰æ­¥éª¤éƒ½æˆåŠŸå®Œæˆï¼")
        print("\\nğŸ“ ä¸‹ä¸€æ­¥:")
        print("1. è¿è¡Œ: python -m uvicorn backend.app:app --host 0.0.0.0 --port 8000")
        print("2. æˆ–è€…ä½¿ç”¨: !python -m uvicorn backend.app:app --host 0.0.0.0 --port 8000")
        print("3. è®¿é—®åº”ç”¨: http://localhost:8000")
    else:
        print("\\nâš ï¸ éƒ¨åˆ†æ­¥éª¤å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
    
    return success_count == len(steps)

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)