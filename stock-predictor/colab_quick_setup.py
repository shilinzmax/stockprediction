#!/usr/bin/env python3
"""
Colabå¿«é€Ÿè®¾ç½®è„šæœ¬
ä¸€é”®è®¾ç½®å’Œå¯åŠ¨è‚¡ç¥¨é¢„æµ‹ç³»ç»Ÿ
"""

import os
import subprocess
import time
import threading
import sys

def run_command(cmd, description=""):
    """è¿è¡Œå‘½ä»¤å¹¶æ˜¾ç¤ºç»“æœ"""
    print(f"ğŸ”„ {description}")
    try:
        result = subprocess.run(cmd, shell=True, check=True, capture_output=True, text=True)
        print(f"âœ… {description} å®Œæˆ")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ {description} å¤±è´¥: {e}")
        return False

def setup_environment():
    """è®¾ç½®ç¯å¢ƒ"""
    print("ğŸš€ å¼€å§‹è®¾ç½®Colabç¯å¢ƒ...")
    
    # å®‰è£…ç³»ç»Ÿä¾èµ–
    commands = [
        ("apt-get update", "æ›´æ–°åŒ…åˆ—è¡¨"),
        ("apt-get install -y curl wget", "å®‰è£…ç³»ç»Ÿå·¥å…·"),
        ("curl -fsSL https://ollama.com/install.sh | sh", "å®‰è£…Ollama")
    ]
    
    for cmd, desc in commands:
        if not run_command(cmd, desc):
            return False
    
    # å®‰è£…Pythonä¾èµ–
    packages = [
        "fastapi", "uvicorn[standard]", "langgraph", "langchain", 
        "langchain-openai", "yfinance", "pandas", "numpy", "ta",
        "pydantic", "python-multipart", "python-dotenv", "httpx",
        "aiofiles", "pyarrow", "alpha-vantage", "requests", "aiohttp", "ollama"
    ]
    
    for package in packages:
        if not run_command(f"pip install -q {package}", f"å®‰è£… {package}"):
            return False
    
    return True

def start_ollama_service():
    """å¯åŠ¨OllamaæœåŠ¡"""
    print("ğŸ¤– å¯åŠ¨OllamaæœåŠ¡...")
    
    def start_ollama():
        subprocess.run(['ollama', 'serve'], check=True)
    
    ollama_thread = threading.Thread(target=start_ollama, daemon=True)
    ollama_thread.start()
    time.sleep(10)
    
    # éªŒè¯æœåŠ¡å¯åŠ¨
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            print("âœ… OllamaæœåŠ¡å¯åŠ¨æˆåŠŸ")
            return True
    except:
        pass
    
    print("âŒ OllamaæœåŠ¡å¯åŠ¨å¤±è´¥")
    return False

def download_model():
    """ä¸‹è½½LLMæ¨¡å‹"""
    print("ğŸ“¥ ä¸‹è½½Qwen2.5æ¨¡å‹...")
    return run_command("ollama pull qwen2.5:7b", "ä¸‹è½½Qwen2.5æ¨¡å‹")

def setup_environment_variables():
    """è®¾ç½®ç¯å¢ƒå˜é‡"""
    print("âš™ï¸ é…ç½®ç¯å¢ƒå˜é‡...")
    
    env_vars = {
        "LLM_TYPE": "ollama",
        "OLLAMA_MODEL": "qwen2.5:7b",
        "HOST": "0.0.0.0",
        "PORT": "8000",
        "CACHE_TTL_HOURS": "24"
    }
    
    for key, value in env_vars.items():
        os.environ[key] = value
        print(f"  {key}={value}")
    
    print("âœ… ç¯å¢ƒå˜é‡é…ç½®å®Œæˆ")
    return True

def clone_project():
    """å…‹éš†é¡¹ç›®"""
    print("ğŸ“¥ å…‹éš†é¡¹ç›®...")
    
    if not run_command("git clone https://github.com/shilinzmax/stockprediction.git", "å…‹éš†é¡¹ç›®"):
        return False
    
    # åˆ‡æ¢åˆ°é¡¹ç›®ç›®å½•
    os.chdir('/content/stockprediction/stock-predictor')
    print("âœ… å·²åˆ‡æ¢åˆ°é¡¹ç›®ç›®å½•")
    return True

def test_ollama():
    """æµ‹è¯•Ollamaè¿æ¥"""
    print("ğŸ§ª æµ‹è¯•Ollamaè¿æ¥...")
    
    try:
        import ollama
        response = ollama.chat(
            model="qwen2.5:7b",
            messages=[{'role': 'user', 'content': 'Hello, are you working?'}]
        )
        print("âœ… Ollamaæµ‹è¯•æˆåŠŸ")
        print(f"å›å¤: {response['message']['content'][:100]}...")
        return True
    except Exception as e:
        print(f"âŒ Ollamaæµ‹è¯•å¤±è´¥: {e}")
        return False

def start_application():
    """å¯åŠ¨åº”ç”¨"""
    print("ğŸš€ å¯åŠ¨è‚¡ç¥¨é¢„æµ‹åº”ç”¨...")
    
    try:
        # ä½¿ç”¨å‘½ä»¤è¡Œå¯åŠ¨
        result = subprocess.run([
            'python', '-m', 'uvicorn', 
            'backend.app:app', 
            '--host', '0.0.0.0', 
            '--port', '8000'
        ], check=True)
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        return False
    
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ¯ è‚¡ç¥¨é¢„æµ‹ç³»ç»Ÿ - Colabå¿«é€Ÿè®¾ç½®")
    print("=" * 60)
    
    # æ‰§è¡Œè®¾ç½®æ­¥éª¤
    steps = [
        ("è®¾ç½®ç¯å¢ƒ", setup_environment),
        ("å¯åŠ¨OllamaæœåŠ¡", start_ollama_service),
        ("ä¸‹è½½LLMæ¨¡å‹", download_model),
        ("é…ç½®ç¯å¢ƒå˜é‡", setup_environment_variables),
        ("å…‹éš†é¡¹ç›®", clone_project),
        ("æµ‹è¯•Ollamaè¿æ¥", test_ollama)
    ]
    
    success_count = 0
    for step_name, step_func in steps:
        print(f"\nğŸ“‹ {step_name}...")
        try:
            if step_func():
                success_count += 1
            else:
                print(f"âŒ {step_name} å¤±è´¥")
        except Exception as e:
            print(f"âŒ {step_name} å¼‚å¸¸: {e}")
    
    print("\n" + "=" * 60)
    print(f"ğŸ‰ è®¾ç½®å®Œæˆï¼æˆåŠŸæ­¥éª¤: {success_count}/{len(steps)}")
    
    if success_count == len(steps):
        print("\nâœ… æ‰€æœ‰æ­¥éª¤éƒ½æˆåŠŸå®Œæˆï¼")
        print("\nğŸ“ ä¸‹ä¸€æ­¥:")
        print("1. è¿è¡Œ: start_application()")
        print("2. æˆ–è€…ä½¿ç”¨: !python -m uvicorn backend.app:app --host 0.0.0.0 --port 8000")
        
        # è¯¢é—®æ˜¯å¦å¯åŠ¨åº”ç”¨
        try:
            response = input("\næ˜¯å¦ç°åœ¨å¯åŠ¨åº”ç”¨ï¼Ÿ(y/n): ")
            if response.lower() in ['y', 'yes', 'æ˜¯']:
                start_application()
        except:
            print("\nğŸ’¡ æ‰‹åŠ¨å¯åŠ¨åº”ç”¨:")
            print("start_application()")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æ­¥éª¤å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
    
    return success_count == len(steps)

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
