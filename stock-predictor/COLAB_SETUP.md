# Google Colab éƒ¨ç½²æŒ‡å—

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æ‰“å¼€Colab Notebook
- è®¿é—® [Google Colab](https://colab.research.google.com/)
- åˆ›å»ºæ–°çš„Python 3 notebook

### 2. å®‰è£…ä¾èµ–
åœ¨ç¬¬ä¸€ä¸ªcellä¸­è¿è¡Œï¼š

```python
# å®‰è£…ç³»ç»Ÿä¾èµ–
!apt-get update
!apt-get install -y curl

# å®‰è£…Ollama
!curl -fsSL https://ollama.com/install.sh | sh

# å®‰è£…Pythonä¾èµ–
!pip install -q fastapi uvicorn langgraph langchain langchain-openai yfinance pandas numpy ta pydantic python-multipart python-dotenv httpx aiofiles pyarrow alpha-vantage requests aiohttp ollama

# å®‰è£…å‰ç«¯ä¾èµ–ï¼ˆå¦‚æœéœ€è¦ï¼‰
!pip install -q jupyter-dash plotly dash
```

### 3. ä¸‹è½½é¡¹ç›®æ–‡ä»¶
```python
# å…‹éš†é¡¹ç›®ï¼ˆå¦‚æœä»GitHubï¼‰
!git clone https://github.com/your-username/stock-predictor.git
%cd stock-predictor

# æˆ–è€…ç›´æ¥ä¸Šä¼ æ–‡ä»¶åˆ°Colab
# ä½¿ç”¨å·¦ä¾§æ–‡ä»¶é¢æ¿ä¸Šä¼ é¡¹ç›®æ–‡ä»¶
```

### 4. å¯åŠ¨OllamaæœåŠ¡
```python
# å¯åŠ¨OllamaæœåŠ¡
import subprocess
import time
import threading

def start_ollama():
    subprocess.run(['ollama', 'serve'], check=True)

# åœ¨åå°å¯åŠ¨Ollama
ollama_thread = threading.Thread(target=start_ollama, daemon=True)
ollama_thread.start()

# ç­‰å¾…æœåŠ¡å¯åŠ¨
time.sleep(5)
print("âœ… OllamaæœåŠ¡å·²å¯åŠ¨")
```

### 5. ä¸‹è½½LLMæ¨¡å‹
```python
# ä¸‹è½½Qwen2.5æ¨¡å‹
!ollama pull qwen2.5:7b

# éªŒè¯æ¨¡å‹
!ollama list
```

### 6. é…ç½®ç¯å¢ƒå˜é‡
```python
import os

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["LLM_TYPE"] = "ollama"
os.environ["OLLAMA_MODEL"] = "qwen2.5:7b"
os.environ["HOST"] = "0.0.0.0"
os.environ["PORT"] = "8000"

print("âœ… ç¯å¢ƒå˜é‡å·²é…ç½®")
```

### 7. æµ‹è¯•ç³»ç»Ÿ
```python
# æµ‹è¯•Ollamaè¿æ¥
import ollama

try:
    response = ollama.chat(
        model="qwen2.5:7b",
        messages=[{'role': 'user', 'content': 'Hello, are you working?'}]
    )
    print("âœ… Ollamaæµ‹è¯•æˆåŠŸ:", response['message']['content'])
except Exception as e:
    print("âŒ Ollamaæµ‹è¯•å¤±è´¥:", str(e))
```

### 8. å¯åŠ¨åº”ç”¨
```python
# å¯åŠ¨FastAPIåº”ç”¨
import uvicorn
from backend.app import app

# åœ¨Colabä¸­å¯åŠ¨æœåŠ¡å™¨
uvicorn.run(app, host="0.0.0.0", port=8000)
```

## ğŸ”§ é«˜çº§é…ç½®

### ä½¿ç”¨GPUåŠ é€Ÿ
```python
# æ£€æŸ¥GPUå¯ç”¨æ€§
import torch
print("CUDAå¯ç”¨:", torch.cuda.is_available())
print("GPUæ•°é‡:", torch.cuda.device_count())

# å¦‚æœä½¿ç”¨GPUç‰ˆæœ¬çš„Ollama
!ollama pull qwen2.5:7b
```

### æŒä¹…åŒ–å­˜å‚¨
```python
# æŒ‚è½½Google Drive
from google.colab import drive
drive.mount('/content/drive')

# å°†é¡¹ç›®ä¿å­˜åˆ°Drive
!cp -r /content/stock-predictor /content/drive/MyDrive/
```

### è‡ªå®šä¹‰æ¨¡å‹
```python
# ä¸‹è½½å…¶ä»–æ¨¡å‹
!ollama pull llama2:7b
!ollama pull mistral:7b

# åˆ‡æ¢æ¨¡å‹
os.environ["OLLAMA_MODEL"] = "llama2:7b"
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### 1. å†…å­˜ç®¡ç†
```python
import gc
import psutil

# ç›‘æ§å†…å­˜ä½¿ç”¨
def check_memory():
    memory = psutil.virtual_memory()
    print(f"å†…å­˜ä½¿ç”¨: {memory.percent}%")
    if memory.percent > 80:
        gc.collect()
        print("å·²æ¸…ç†å†…å­˜")

# å®šæœŸæ£€æŸ¥å†…å­˜
import time
while True:
    check_memory()
    time.sleep(60)
```

### 2. ç¼“å­˜ä¼˜åŒ–
```python
# è®¾ç½®æ›´å¤§çš„ç¼“å­˜
os.environ["CACHE_TTL_HOURS"] = "24"
os.environ["CACHE_DIR"] = "/content/drive/MyDrive/stock-cache"
```

### 3. å¹¶å‘æ§åˆ¶
```python
# é™åˆ¶å¹¶å‘è¯·æ±‚
os.environ["MAX_CONCURRENT_REQUESTS"] = "5"
```

## ğŸš¨ æ³¨æ„äº‹é¡¹

### 1. Colabé™åˆ¶
- ä¼šè¯æ—¶é—´é™åˆ¶ï¼š12å°æ—¶
- å†…å­˜é™åˆ¶ï¼š12GB RAM
- å­˜å‚¨é™åˆ¶ï¼šä¸´æ—¶å­˜å‚¨ä¼šåœ¨ä¼šè¯ç»“æŸåæ¸…é™¤

### 2. æœ€ä½³å®è·µ
- å®šæœŸä¿å­˜å·¥ä½œåˆ°Google Drive
- ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ä»¥èŠ‚çœå†…å­˜
- ç›‘æ§èµ„æºä½¿ç”¨æƒ…å†µ
- è®¾ç½®è‡ªåŠ¨é‡å¯æœºåˆ¶

### 3. æ•…éšœæ’é™¤
```python
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
!ps aux | grep ollama

# é‡å¯Ollama
!pkill ollama
!ollama serve &

# æ£€æŸ¥ç«¯å£
!netstat -tlnp | grep 8000
```

## ğŸ“± è®¿é—®åº”ç”¨

å¯åŠ¨åï¼Œä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¿é—®ï¼š
- æœ¬åœ°è®¿é—®ï¼š`http://localhost:8000`
- å…¬å…±è®¿é—®ï¼šä½¿ç”¨Colabçš„å…¬å…±URLåŠŸèƒ½

## ğŸ”„ è‡ªåŠ¨åŒ–è„šæœ¬

åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„å¯åŠ¨è„šæœ¬ï¼š

```python
# å®Œæ•´å¯åŠ¨è„šæœ¬
def setup_colab_environment():
    """è®¾ç½®Colabç¯å¢ƒ"""
    print("ğŸš€ å¼€å§‹è®¾ç½®Colabç¯å¢ƒ...")
    
    # å®‰è£…ä¾èµ–
    print("ğŸ“¦ å®‰è£…ä¾èµ–...")
    !pip install -q fastapi uvicorn langgraph langchain langchain-openai yfinance pandas numpy ta pydantic python-multipart python-dotenv httpx aiofiles pyarrow alpha-vantage requests aiohttp ollama
    
    # å¯åŠ¨Ollama
    print("ğŸ¤– å¯åŠ¨Ollama...")
    import subprocess
    import threading
    import time
    
    def start_ollama():
        subprocess.run(['ollama', 'serve'], check=True)
    
    ollama_thread = threading.Thread(target=start_ollama, daemon=True)
    ollama_thread.start()
    time.sleep(5)
    
    # ä¸‹è½½æ¨¡å‹
    print("ğŸ“¥ ä¸‹è½½æ¨¡å‹...")
    !ollama pull qwen2.5:7b
    
    # é…ç½®ç¯å¢ƒ
    print("âš™ï¸ é…ç½®ç¯å¢ƒ...")
    import os
    os.environ["LLM_TYPE"] = "ollama"
    os.environ["OLLAMA_MODEL"] = "qwen2.5:7b"
    
    print("âœ… ç¯å¢ƒè®¾ç½®å®Œæˆï¼")

# è¿è¡Œè®¾ç½®
setup_colab_environment()
```

è¿™æ ·ä½ å°±å¯ä»¥åœ¨Colabä¸Šé«˜æ•ˆè¿è¡Œä½ çš„è‚¡ç¥¨é¢„æµ‹ç³»ç»Ÿäº†ï¼
