#!/usr/bin/env python3
"""
Colabå¿«é€Ÿå¯åŠ¨è„šæœ¬
ä¸€é”®è®¾ç½®è‚¡ç¥¨é¢„æµ‹ç³»ç»Ÿåœ¨Colabä¸­è¿è¡Œ
"""

import os
import subprocess
import time
import threading

def run_command(cmd, description=""):
    """è¿è¡Œå‘½ä»¤å¹¶æ˜¾ç¤ºç»“æœ"""
    print(f"ğŸ”„ {description}")
    try:
        result = subprocess.run(cmd, shell=True, check=True, capture_output=True, text=True)
        print(f"âœ… {description} å®Œæˆ")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ {description} å¤±è´¥: {e}")
        return False

def setup_colab():
    """è®¾ç½®Colabç¯å¢ƒ"""
    print("ğŸš€ å¼€å§‹è®¾ç½®Colabç¯å¢ƒ...")
    
    # 1. å®‰è£…ç³»ç»Ÿä¾èµ–
    commands = [
        ("apt-get update", "æ›´æ–°åŒ…åˆ—è¡¨"),
        ("apt-get install -y curl", "å®‰è£…curl"),
        ("curl -fsSL https://ollama.com/install.sh | sh", "å®‰è£…Ollama")
    ]
    
    for cmd, desc in commands:
        if not run_command(cmd, desc):
            return False
    
    # 2. å®‰è£…Pythonä¾èµ–
    packages = [
        "fastapi", "uvicorn[standard]", "langgraph", "langchain", 
        "langchain-openai", "yfinance", "pandas", "numpy", "ta",
        "pydantic", "python-multipart", "python-dotenv", "httpx",
        "aiofiles", "pyarrow", "alpha-vantage", "requests", "aiohttp", "ollama"
    ]
    
    for package in packages:
        if not run_command(f"pip install -q {package}", f"å®‰è£… {package}"):
            return False
    
    # 3. å¯åŠ¨OllamaæœåŠ¡
    print("ğŸ¤– å¯åŠ¨OllamaæœåŠ¡...")
    
    def start_ollama():
        subprocess.run(['ollama', 'serve'], check=True)
    
    ollama_thread = threading.Thread(target=start_ollama, daemon=True)
    ollama_thread.start()
    time.sleep(10)
    
    # 4. ä¸‹è½½æ¨¡å‹
    if not run_command("ollama pull qwen2.5:7b", "ä¸‹è½½Qwen2.5æ¨¡å‹"):
        return False
    
    # 5. è®¾ç½®ç¯å¢ƒå˜é‡
    print("âš™ï¸ é…ç½®ç¯å¢ƒå˜é‡...")
    os.environ["LLM_TYPE"] = "ollama"
    os.environ["OLLAMA_MODEL"] = "qwen2.5:7b"
    os.environ["HOST"] = "0.0.0.0"
    os.environ["PORT"] = "8000"
    
    print("âœ… ç¯å¢ƒè®¾ç½®å®Œæˆï¼")
    return True

def test_system():
    """æµ‹è¯•ç³»ç»Ÿ"""
    print("ğŸ§ª æµ‹è¯•ç³»ç»Ÿ...")
    
    try:
        import ollama
        response = ollama.chat(
            model="qwen2.5:7b",
            messages=[{'role': 'user', 'content': 'Hello'}]
        )
        print("âœ… ç³»ç»Ÿæµ‹è¯•æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
        return False

def start_app():
    """å¯åŠ¨åº”ç”¨"""
    print("ğŸš€ å¯åŠ¨è‚¡ç¥¨é¢„æµ‹åº”ç”¨...")
    
    try:
        import uvicorn
        from backend.app import app
        
        print("ğŸ“± åº”ç”¨å°†åœ¨ http://localhost:8000 å¯åŠ¨")
        print("ğŸ“Š APIæ–‡æ¡£: http://localhost:8000/docs")
        
        uvicorn.run(app, host="0.0.0.0", port=8000)
        
    except Exception as e:
        print(f"âŒ åº”ç”¨å¯åŠ¨å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ¯ è‚¡ç¥¨é¢„æµ‹ç³»ç»Ÿ - Colabå¿«é€Ÿå¯åŠ¨")
    print("=" * 60)
    
    # è®¾ç½®ç¯å¢ƒ
    if setup_colab():
        # æµ‹è¯•ç³»ç»Ÿ
        if test_system():
            # å¯åŠ¨åº”ç”¨
            start_app()
        else:
            print("âŒ ç³»ç»Ÿæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
    else:
        print("âŒ ç¯å¢ƒè®¾ç½®å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
